{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGcMr8gSoOELGJleOBAMHk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shirley-333/intro-final-project_Xin-Wen/blob/main/final_project_Xin_Wen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abstract**\n",
        "\n",
        "Lofi study music is one of the largest, most prominent music genres to emerge online in the past decade. Between streams of the “Lofi Girl” and countless “chill beats” playlists, lofi music is now synonymous with late-night study sessions and the student experience. I wanted to investigate the dynamics of this community. Are students documenting their academic stresses, nostalgia, or motivation? Is the comment section that is associated with lofi music just a uniquely supportive one? In order to answer this question, I scraped several lists of lofi study music from YouTube and ran those comments through various analyses, using several techniques explored in class (APIs, JSON, Pandas, sentiment analysis, and TF-IDF).\n",
        "\n"
      ],
      "metadata": {
        "id": "qsSVFVyBa4ln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Collection and cleaning**\n",
        "\n",
        "First, I used the YouTube Data API to collect the comments and metadata of videos identified by the search queries “lofi study music” and “lofi beats to relax.” A helper function was used to make the request to the API endpoint using the requests package, which returned a JSON object. Then I used Pandas to wrangle the JSON object into a DataFrame and conduct an inner join to combine all the comment data. This process of wrangling the JSON objects from each page of extracted comments continued until the nextPageToken returned no value. Finally, I applied a series of cleaning functions to the DataFrame, which resulted in several thousand comments across multiple videos for further analyses, including sentiment analysis and keyword frequency count."
      ],
      "metadata": {
        "id": "h6e99Lfub2Yt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24goFXs7eCoS"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade nltk requests tqdm\n",
        "\n",
        "import os\n",
        "import json\n",
        "from urllib.parse import urlencode\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"YOUTUBE_API_KEY\"] = getpass(\"Paste your YouTube API key: \")\n",
        "\n",
        "API_KEY = os.environ.get(\"YOUTUBE_API_KEY\")\n",
        "assert API_KEY, \"API key not set!\""
      ],
      "metadata": {
        "id": "_HNGmBDHfQft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_URL = \"https://www.googleapis.com/youtube/v3\"\n",
        "\n",
        "def yt_get(resource: str, params: dict) -> dict:\n",
        "    q = {**params, \"key\": API_KEY}\n",
        "    url = f\"{BASE_URL}/{resource}?{urlencode(q)}\"\n",
        "    r = requests.get(url, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    return r.json()"
      ],
      "metadata": {
        "id": "jloMIAzggMNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = \"lofi study music\"\n",
        "TARGET_VIDEOS = 30\n",
        "MAX_RESULTS = 50\n",
        "\n",
        "video_hits = []\n",
        "page_token = None\n",
        "\n",
        "with tqdm(total=TARGET_VIDEOS, desc=\"Searching videos\") as pbar:\n",
        "    while len(video_hits) < TARGET_VIDEOS:\n",
        "        params = {\n",
        "            \"part\": \"snippet\",\n",
        "            \"q\": QUERY,\n",
        "            \"type\": \"video\",\n",
        "            \"maxResults\": MAX_RESULTS,\n",
        "            \"order\": \"relevance\",\n",
        "        }\n",
        "        if page_token:\n",
        "            params[\"pageToken\"] = page_token\n",
        "\n",
        "        data = yt_get(\"search\", params)\n",
        "        items = data.get(\"items\", [])\n",
        "\n",
        "        for it in items:\n",
        "            vid = it.get(\"id\", {}).get(\"videoId\")\n",
        "            if not vid:\n",
        "                continue\n",
        "\n",
        "            snip = it.get(\"snippet\", {})\n",
        "            video_hits.append({\n",
        "                \"video_id\": vid,\n",
        "                \"title\": snip.get(\"title\"),\n",
        "                \"channelTitle\": snip.get(\"channelTitle\"),\n",
        "                \"publishedAt\": snip.get(\"publishedAt\"),\n",
        "            })\n",
        "            pbar.update(1)\n",
        "            if len(video_hits) >= TARGET_VIDEOS:\n",
        "                break\n",
        "\n",
        "        page_token = data.get(\"nextPageToken\")\n",
        "        if not page_token:\n",
        "            break\n",
        "\n",
        "videos_df = pd.DataFrame(video_hits)\n",
        "videos_df.head()"
      ],
      "metadata": {
        "id": "YcOKTItCgjOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_comments = []\n",
        "\n",
        "for vid in tqdm(videos_df[\"video_id\"].tolist(), desc=\"Fetching comments\"):\n",
        "    page_token = None\n",
        "    fetched = 0\n",
        "    try:\n",
        "        while True:\n",
        "            params = {\n",
        "                \"part\": \"snippet\",\n",
        "                \"videoId\": vid,\n",
        "                \"maxResults\": 100,\n",
        "                \"order\": \"relevance\"\n",
        "            }\n",
        "            if page_token:\n",
        "                params[\"pageToken\"] = page_token\n",
        "\n",
        "            data = yt_get(\"commentThreads\", params)\n",
        "            items = data.get(\"items\", [])\n",
        "\n",
        "            for it in items:\n",
        "                top = it.get(\"snippet\", {}).get(\"topLevelComment\", {})\n",
        "                s = top.get(\"snippet\", {})\n",
        "                all_comments.append({\n",
        "                    \"video_id\": vid,\n",
        "                    \"author\": s.get(\"authorDisplayName\"),\n",
        "                    \"publishedAt\": s.get(\"publishedAt\"),\n",
        "                    \"likeCount\": s.get(\"likeCount\", 0),\n",
        "                    \"text\": s.get(\"textOriginal\", \"\"),\n",
        "                })\n",
        "                fetched += 1\n",
        "\n",
        "            page_token = data.get(\"nextPageToken\")\n",
        "            if not page_token or fetched >= 300:\n",
        "                break\n",
        "\n",
        "    except requests.HTTPError as e:\n",
        "        print(f\"Skipping {vid} due to HTTP error: {e}\")\n",
        "        continue\n",
        "\n",
        "comments_df = pd.DataFrame(all_comments)\n",
        "comments_df.head()"
      ],
      "metadata": {
        "id": "bN85Bw6IiNIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used VADER for the sentiment analysis. VADER is suitable for analyzing social media content such as YouTube comments and works well with short text and emojis. Each comment received a compound score which could be interpreted as positive, neutral, or negative feedback. The results of the analysis revealed a strong bias. The majority of the comments were positive. Many users shared their personal thoughts publicly with statements such as “This helps me get through the day” or “Sending love to anyone studying right now.” Others left short and direct messages such as “beautiful,” “thank you,” and “I love this track.” This pattern was consistent across all the musical pieces in the sample.\n",
        "\n",
        "A basic sentiment analysis is a good place to start. The bar chart shows that most of the comments are positive, some are neutral, and very few are negative. Although this is a very simple visualization, the results are illuminating. Most lofi chillhop fans are using YouTube comments to express praise, excitement, and empathy. This can be explained by the purpose of the"
      ],
      "metadata": {
        "id": "guzLydS_g28W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compound_score(text: str) -> float:\n",
        "    return sia.polarity_scores(text or \"\")[\"compound\"]\n",
        "\n",
        "comments_df[\"compound\"] = comments_df[\"text\"].fillna(\"\").apply(compound_score)\n",
        "comments_df[[\"text\", \"compound\"]].head()"
      ],
      "metadata": {
        "id": "i6qnBf_2ilTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_sentiment(c):\n",
        "    if c >= 0.05:\n",
        "        return \"Positive\"\n",
        "    elif c <= -0.05:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "comments_df[\"sentiment\"] = comments_df[\"compound\"].apply(label_sentiment)\n",
        "comments_df[[\"text\", \"compound\", \"sentiment\"]].head()"
      ],
      "metadata": {
        "id": "-U9dTFUgiqMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sent_counts = comments_df[\"sentiment\"].value_counts().reindex(\n",
        "    [\"Positive\", \"Neutral\", \"Negative\"]\n",
        ")\n",
        "sent_counts = sent_counts.fillna(0)\n",
        "\n",
        "sent_share = sent_counts / sent_counts.sum()\n",
        "\n",
        "print(\"Counts:\\n\", sent_counts)\n",
        "print(\"\\nShare:\\n\", sent_share.round(3))\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sent_counts.plot(kind=\"bar\")\n",
        "\n",
        "plt.title(\"Sentiment of YouTube Comments on Lofi Study Music\")\n",
        "plt.ylabel(\"Number of Comments\")\n",
        "plt.xlabel(\"Sentiment Category\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-4M13FlYix2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I then examined the changes in sentiment across time, using year as a variable from 2019 to 2025. The earlier result holds in this distribution, with positive comments being dominant across all years. The proportion of neutral comments fluctuates more heavily between years than with the previous variables but it remains the second largest sentiment category observed. Similarly, negative comments are minimal to nonexistent across all years. The data here suggests that there is a stability of sentiment within this lofi community, in that no matter what year the comments were scraped from, the reaction is the same. This is particularly interesting given the changes in academic calendars and years, and wider social events that might affect people’s emotions.\n",
        "\n",
        "I ran a TF-IDF analysis on the comments to get more insight on the meaning embedded in the comments. The most significant words are love, music, thank, study, work, reading, and beautiful. The narrative embedded in these words suggests that lofi beats mean more than a general music genre. The music, within the context of lofi, speaks to a larger value system of productivity, daily rituals, and gratitude. The positive, repetitive, and familiar words align with the concept and style of lofi music, which is familiar, warm, and recognizable.\n",
        "\n",
        "Another theme that I observed in these comments was the conspicuous amount of positive wishes extended to strangers. Many viewers wrote things like “If you’re studying I hope you pass your exam!” The comment section in these lofi Hip-Hop videos serves as a micro community to exchange kind words, personal stories, and well-wishes. Perhaps these comments are also a reason why millions of people return to these videos time and time again. They provide more than tastefully chopped song samples paired with anime-like illustrations. These videos also provide their viewers with a unique platform to share intimate details about themselves in a non-intrusive and kind-spirited environment.\n"
      ],
      "metadata": {
        "id": "musU9pfweNtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comments_df[\"publishedAt\"] = pd.to_datetime(comments_df[\"publishedAt\"], errors=\"coerce\")\n",
        "comments_df[\"year\"] = comments_df[\"publishedAt\"].dt.year\n",
        "comments_df.head()"
      ],
      "metadata": {
        "id": "QQt9-xIclrKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "year_sent = (\n",
        "    comments_df\n",
        "    .groupby([\"year\", \"sentiment\"])\n",
        "    .size()\n",
        "    .reset_index(name=\"count\")\n",
        ")\n",
        "\n",
        "year_totals = (\n",
        "    comments_df.groupby(\"year\")\n",
        "    .size()\n",
        "    .reset_index(name=\"total\")\n",
        ")\n",
        "\n",
        "year_sent = year_sent.merge(year_totals, on=\"year\")\n",
        "year_sent[\"proportion\"] = year_sent[\"count\"] / year_sent[\"total\"]\n",
        "year_sent.head()"
      ],
      "metadata": {
        "id": "_6_88TTLlt05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot = year_sent.pivot(index=\"year\", columns=\"sentiment\", values=\"proportion\").fillna(0)\n",
        "pivot"
      ],
      "metadata": {
        "id": "ZRNSU5RllvAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "\n",
        "for col in [\"Positive\", \"Neutral\", \"Negative\"]:\n",
        "    if col in pivot.columns:\n",
        "        plt.plot(pivot.index, pivot[col], marker=\"o\", label=col)\n",
        "\n",
        "plt.title(\"Sentiment Trends in Lofi Study Music Comments Over Time\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Proportion of Comments\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QdTNIOE4lzlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project has some limitations. Firstly, due to the restrictions of the YouTube API, the comments and replies that were obtained were not comprehensive. Secondly, VADER has limitations in terms of accurately detecting nuanced sentiments, such as sarcasm, and it also has limitations in multilingual sentiment analysis. Lastly, using TF-IDF as a tool for topic detection has its limitations because it cannot directly present the themes discussed within a corpus of text and does not provide insights into the emotional content of the texts. Future studies could add to these findings by collecting a wider dataset of YouTube comments or including another form of engagement such as live comments, and finally utilize other machine learning approaches such as topic modeling to better understand discussions and themes mentioned by content creators and viewers.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "This project led me to reflect on how communities within YouTube can be a source of emotional support. The community of listeners in lofi study music is ultimately filled with positive interactions between people who encourage one another and express communal appreciation for simple things. By using the techniques learned in class to analyze this simple trend that I hoped to understand, I was able to learn about some technical components of data scraping, working with APIs and JSON files, and analyzing human responses through messaging to understand not only their uses for music but also their respective community dynamics."
      ],
      "metadata": {
        "id": "qm2f5UE4fBKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "github URL: https://github.com/Shirley-333/intro-final-project_Xin-Wen/blob/main/final_project_Xin_Wen.ipynb"
      ],
      "metadata": {
        "id": "gV131zSB2nOP"
      }
    }
  ]
}